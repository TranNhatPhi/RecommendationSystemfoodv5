#!/usr/bin/env python3
"""
Simple RAG system with GPU + ChromaDB + LangChain
Focuses on loading CSV and basic RAG functionality
"""

import chromadb
from sentence_transformers import SentenceTransformer
import pandas as pd
import torch
import json
import time
from typing import List, Dict, Any
from datetime import datetime

class SimpleRAGSystem:
    def __init__(self, use_gpu: bool = True):
        """Initialize simple RAG system with GPU support"""
        self.use_gpu = use_gpu and torch.cuda.is_available()
        
        # Setup device
        self.device = 'cuda' if self.use_gpu else 'cpu'
        print(f"üöÄ Initializing RAG on device: {self.device}")
        
        if self.use_gpu:
            print(f"üéØ GPU: {torch.cuda.get_device_name(0)}")
            print(f"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory // 1024**3} GB")
        
        # Initialize embedding model
        self.embedding_model = SentenceTransformer(
            'paraphrase-multilingual-mpnet-base-v2',
            device=self.device
        )
        
        # Initialize ChromaDB (in-memory for reliability)
        self.client = chromadb.Client()
        self.collections = {}
        
        print("‚úÖ Simple RAG system initialized")
    
    def load_csv_data(self, csv_path: str, collection_name: str, 
                     text_columns: List[str], id_column: str = None,
                     batch_size: int = 32) -> Dict[str, Any]:
        """Load CSV data into ChromaDB collection"""
        
        print(f"üìä Loading CSV: {csv_path}")
        
        try:
            # Load CSV
            df = pd.read_csv(csv_path)
            print(f"üìà Loaded {len(df)} rows")
            
            # Create collection
            if collection_name in self.collections:
                self.client.delete_collection(collection_name)
            
            collection = self.client.create_collection(collection_name)
            self.collections[collection_name] = collection
            
            # Prepare data
            documents = []
            metadatas = []
            ids = []
            
            for idx, row in df.iterrows():
                # Combine text columns
                text_parts = []
                for col in text_columns:
                    if col in df.columns and pd.notna(row[col]):
                        text_parts.append(str(row[col]))
                
                if not text_parts:
                    continue
                
                document = " | ".join(text_parts)
                documents.append(document)
                
                # Create metadata from all other columns
                metadata = {}
                for col in df.columns:
                    if col not in text_columns and pd.notna(row[col]):
                        metadata[col] = str(row[col])
                
                metadatas.append(metadata)
                
                # Generate ID
                if id_column and id_column in df.columns:
                    doc_id = str(row[id_column])
                else:
                    doc_id = f"{collection_name}_{idx}"
                ids.append(doc_id)
            
            print(f"üìù Prepared {len(documents)} documents")
            
            # Process in batches
            total_added = 0
            for i in range(0, len(documents), batch_size):
                batch_end = min(i + batch_size, len(documents))
                batch_docs = documents[i:batch_end]
                batch_metas = metadatas[i:batch_end]
                batch_ids = ids[i:batch_end]
                
                print(f"‚ö° Processing batch {i//batch_size + 1}/{(len(documents) + batch_size - 1)//batch_size}")
                
                # Generate embeddings
                start_time = time.time()
                embeddings = self.embedding_model.encode(
                    batch_docs,
                    convert_to_tensor=False,
                    show_progress_bar=False
                )
                embed_time = time.time() - start_time
                
                # Add to collection
                start_time = time.time()
                collection.add(
                    documents=batch_docs,
                    embeddings=embeddings.tolist(),
                    metadatas=batch_metas,
                    ids=batch_ids
                )
                add_time = time.time() - start_time
                
                total_added += len(batch_docs)
                print(f"  ‚úÖ Embedded in {embed_time:.2f}s, Added in {add_time:.2f}s")
                
                # Clear GPU cache
                if self.use_gpu:
                    torch.cuda.empty_cache()
            
            print(f"‚úÖ Loaded {total_added} documents to collection '{collection_name}'")
            
            return {
                'success': True,
                'collection_name': collection_name,
                'total_documents': total_added,
                'source_file': csv_path
            }
            
        except Exception as e:
            print(f"‚ùå Error loading CSV: {str(e)}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def search(self, query: str, collection_name: str, n_results: int = 5) -> Dict[str, Any]:
        """Search in a specific collection"""
        try:
            if collection_name not in self.collections:
                return {'success': False, 'error': f'Collection {collection_name} not found'}
            
            collection = self.collections[collection_name]
            
            # Generate query embedding
            query_embedding = self.embedding_model.encode([query], convert_to_tensor=False)
            
            # Search
            results = collection.query(
                query_embeddings=query_embedding.tolist(),
                n_results=n_results
            )
            
            return {
                'success': True,
                'query': query,
                'collection': collection_name,
                'results': results
            }
            
        except Exception as e:
            print(f"‚ùå Search error: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def multi_collection_search(self, query: str, collections: List[str] = None, 
                              n_results_per_collection: int = 3) -> Dict[str, Any]:
        """Search across multiple collections"""
        if collections is None:
            collections = list(self.collections.keys())
        
        all_results = {}
        
        for collection_name in collections:
            if collection_name in self.collections:
                result = self.search(query, collection_name, n_results_per_collection)
                if result['success']:
                    all_results[collection_name] = result['results']
        
        return {
            'success': True,
            'query': query,
            'collections_searched': collections,
            'results': all_results
        }
    
    def generate_rag_response(self, query: str, collections: List[str] = None) -> Dict[str, Any]:
        """Generate RAG response using retrieved context"""
        
        # Search for relevant context
        search_results = self.multi_collection_search(query, collections, n_results_per_collection=3)
        
        if not search_results['success']:
            return {
                'success': False,
                'error': 'Failed to retrieve context'
            }
        
        # Build context from search results
        context_parts = []
        retrieved_docs = []
        
        for collection_name, results in search_results['results'].items():
            if results['documents'][0]:
                context_parts.append(f"\n=== TH√îNG TIN T·ª™ {collection_name.upper()} ===")
                for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0])):
                    context_parts.append(f"{i+1}. {doc}")
                    retrieved_docs.append({
                        'collection': collection_name,
                        'document': doc,
                        'metadata': metadata
                    })
        
        context = "\n".join(context_parts)
        
        # Generate response (fallback since no OpenAI key)
        response = self._generate_fallback_response(query, context)
        
        return {
            'success': True,
            'query': query,
            'context': context,
            'response': response,
            'retrieved_documents': retrieved_docs,
            'source': 'rag_fallback'
        }
    
    def _generate_fallback_response(self, query: str, context: str) -> str:
        """Generate fallback response based on context and query patterns"""
        
        query_lower = query.lower()
        
        response = "üîç **D·ª±a tr√™n th√¥ng tin t√¨m ƒë∆∞·ª£c:**\n\n"
        
        # Pattern-based responses
        if "gi·∫£m c√¢n" in query_lower:
            response += "üèÉ‚Äç‚ôÄÔ∏è **G·ª£i √Ω cho vi·ªác gi·∫£m c√¢n:**\n"
            response += "‚Ä¢ Ch·ªçn m√≥n ƒÉn √≠t calo, nhi·ªÅu ch·∫•t x∆°\n"
            response += "‚Ä¢ TƒÉng c∆∞·ªùng rau xanh, protein n·∫°c\n"
            response += "‚Ä¢ H·∫°n ch·∫ø ƒë·ªì chi√™n, ng·ªçt, ƒë·ªì u·ªëng c√≥ calo cao\n"
            response += "‚Ä¢ Chia nh·ªè b·ªØa ƒÉn, ƒÉn ch·∫≠m, nhai k·ªπ\n\n"
        
        elif "tƒÉng c√¢n" in query_lower:
            response += "üí™ **G·ª£i √Ω cho vi·ªác tƒÉng c√¢n l√†nh m·∫°nh:**\n"
            response += "‚Ä¢ TƒÉng l∆∞·ª£ng calo v·ªõi th·ª±c ph·∫©m b·ªï d∆∞·ª°ng\n"
            response += "‚Ä¢ Nhi·ªÅu protein, carbohydrate ph·ª©c t·∫°p\n"
            response += "‚Ä¢ ƒÇn nhi·ªÅu b·ªØa nh·ªè trong ng√†y\n"
            response += "‚Ä¢ Th√™m h·∫°t, b∆°, s·ªØa v√†o kh·∫©u ph·∫ßn\n\n"
        
        elif "ti·ªÉu ƒë∆∞·ªùng" in query_lower:
            response += "ü©∫ **Cho ng∆∞·ªùi ti·ªÉu ƒë∆∞·ªùng:**\n"
            response += "‚Ä¢ Ch·ªçn th·ª±c ph·∫©m ch·ªâ s·ªë ƒë∆∞·ªùng huy·∫øt th·∫•p\n"
            response += "‚Ä¢ H·∫°n ch·∫ø ƒë∆∞·ªùng, carbohydrate ƒë∆°n gi·∫£n\n"
            response += "‚Ä¢ TƒÉng ch·∫•t x∆° t·ª´ rau qu·∫£\n"
            response += "‚Ä¢ ƒÇn ƒë·ªÅu ƒë·∫∑n, kh√¥ng b·ªè b·ªØa\n\n"
        
        elif "cao huy·∫øt √°p" in query_lower:
            response += "‚ù§Ô∏è **Cho ng∆∞·ªùi cao huy·∫øt √°p:**\n"
            response += "‚Ä¢ H·∫°n ch·∫ø mu·ªëi v√† natri\n"
            response += "‚Ä¢ TƒÉng th·ª±c ph·∫©m gi√†u kali (chu·ªëi, rau xanh)\n"
            response += "‚Ä¢ Ch·ªçn th·ª±c ph·∫©m t∆∞∆°i, h·∫°n ch·∫ø ch·∫ø bi·∫øn s·∫µn\n"
            response += "‚Ä¢ TƒÉng omega-3 t·ª´ c√°\n\n"
        
        if context and len(context.strip()) > 50:
            response += "üìö **Th√¥ng tin t·ª´ c∆° s·ªü d·ªØ li·ªáu:**\n"
            # Extract relevant info from context
            if "m√≥n" in context.lower():
                response += "‚Ä¢ H·ªá th·ªëng t√¨m th·∫•y c√°c m√≥n ƒÉn ph√π h·ª£p trong d·ªØ li·ªáu\n"
            if "kh√°ch h√†ng" in context.lower():
                response += "‚Ä¢ C√≥ th√¥ng tin v·ªÅ s·ªü th√≠ch c·ªßa kh√°ch h√†ng t∆∞∆°ng t·ª±\n"
            response += f"‚Ä¢ Chi ti·∫øt: {context[:200]}...\n\n"
        
        response += "üí° **L∆∞u √Ω:** ƒê√¢y l√† t∆∞ v·∫•n t·ªïng qu√°t. N√™n tham kh·∫£o √Ω ki·∫øn b√°c sƒ© cho t∆∞ v·∫•n c√° nh√¢n h√≥a."
        
        return response
    
    def get_stats(self) -> Dict[str, Any]:
        """Get system statistics"""
        stats = {
            'gpu_enabled': self.use_gpu,
            'device': self.device,
            'collections': {},
            'total_documents': 0
        }
        
        for name, collection in self.collections.items():
            count = collection.count()
            stats['collections'][name] = count
            stats['total_documents'] += count
        
        return stats

def demo_rag_system():
    """Demo the RAG system"""
    print("üöÄ RAG System Demo with GPU + ChromaDB + LangChain")
    
    # Initialize system
    rag = SimpleRAGSystem(use_gpu=True)
    
    # Test with available CSV files
    csv_files = [
        ('customers_data.csv', 'customers', ['full_name', 'region'], 'customer_id'),
        ('food_recipes_enhanced.csv', 'recipes', ['recipe_name', 'difficulty', 'meal_time', 'nutrition_category'], None),
        ('customer_interactions_enhanced.csv', 'interactions', ['recipe_name', 'difficulty', 'meal_time', 'nutrition_category'], None)
    ]
    
    # Load available CSV files
    loaded_collections = []
    for csv_file, collection_name, text_cols, id_col in csv_files:
        try:
            result = rag.load_csv_data(csv_file, collection_name, text_cols, id_col, batch_size=16)
            if result['success']:
                loaded_collections.append(collection_name)
                print(f"‚úÖ Loaded {collection_name}")
            else:
                print(f"‚ùå Failed to load {collection_name}: {result.get('error', 'Unknown error')}")
        except Exception as e:
            print(f"‚ö†Ô∏è Skipping {csv_file}: {str(e)}")
    
    if not loaded_collections:
        print("‚ö†Ô∏è No data loaded, creating sample data...")
        # Create sample data
        sample_data = pd.DataFrame([
            {"recipe_name": "Ph·ªü b√≤", "difficulty": "medium", "meal_time": "breakfast", "nutrition_category": "protein"},
            {"recipe_name": "B√°nh m√¨", "difficulty": "easy", "meal_time": "breakfast", "nutrition_category": "carbs"},
            {"recipe_name": "C∆°m t·∫•m", "difficulty": "medium", "meal_time": "lunch", "nutrition_category": "balanced"},
            {"recipe_name": "B√∫n ch·∫£", "difficulty": "hard", "meal_time": "lunch", "nutrition_category": "protein"},
            {"recipe_name": "G·ªèi cu·ªën", "difficulty": "easy", "meal_time": "snack", "nutrition_category": "light"}
        ])
        sample_data.to_csv('sample_recipes.csv', index=False)
        
        result = rag.load_csv_data('sample_recipes.csv', 'recipes', ['recipe_name', 'difficulty', 'meal_time', 'nutrition_category'])
        if result['success']:
            loaded_collections.append('recipes')
    
    # Test queries
    if loaded_collections:
        print(f"\nüîç Testing queries on collections: {loaded_collections}")
        
        test_queries = [
            "m√≥n ƒÉn s√°ng Vi·ªát Nam",
            "m√≥n ƒÉn gi·∫£m c√¢n √≠t calo",
            "kh√°ch h√†ng ·ªü H√† N·ªôi",
            "m√≥n ƒÉn cho ng∆∞·ªùi ti·ªÉu ƒë∆∞·ªùng",
            "b·ªØa tr∆∞a b·ªï d∆∞·ª°ng"
        ]
        
        for query in test_queries:
            print(f"\n‚ùì **Query:** {query}")
            
            # Test search
            search_result = rag.multi_collection_search(query, loaded_collections, n_results_per_collection=2)
            if search_result['success']:
                print("üîç **Search Results:**")
                for coll_name, results in search_result['results'].items():
                    if results['documents'][0]:
                        print(f"  üìö {coll_name}:")
                        for i, doc in enumerate(results['documents'][0][:2]):
                            print(f"    {i+1}. {doc[:80]}...")
            
            # Test RAG response
            rag_result = rag.generate_rag_response(query, loaded_collections)
            if rag_result['success']:
                print("ü§ñ **RAG Response:**")
                print(f"  {rag_result['response'][:200]}...")
    
    # Show stats
    print(f"\nüìä **System Statistics:**")
    stats = rag.get_stats()
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    print("\n‚úÖ RAG Demo completed!")
    return rag

if __name__ == "__main__":
    rag_system = demo_rag_system()
