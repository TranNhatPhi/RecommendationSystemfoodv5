from langchain.schema import Document
from langchain.vectorstores.base import VectorStore
from langchain.embeddings.base import Embeddings
from langchain.schema.retriever import BaseRetriever
from langchain.schema import BaseRetriever as LangChainBaseRetriever
from langchain.callbacks.manager import CallbackManagerForRetrieverRun
from langchain.schema.document import Document
from typing import List, Dict, Any, Optional
from rag_vector_db import RAGVectorDatabase
import numpy as np

class ChromaDBEmbeddings(Embeddings):
    """Custom embeddings class for ChromaDB integration with LangChain"""
    
    def __init__(self, rag_db: RAGVectorDatabase):
        self.rag_db = rag_db
        self.embedding_model = rag_db.embedding_model
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Embed a list of documents"""
        return self.embedding_model.encode(texts, convert_to_tensor=False).tolist()
    
    def embed_query(self, text: str) -> List[float]:
        """Embed a single query"""
        return self.embedding_model.encode([text], convert_to_tensor=False)[0].tolist()

class ChromaDBRetriever(LangChainBaseRetriever):
    """Custom retriever for ChromaDB integration with LangChain"""
    
    def __init__(self, rag_db: RAGVectorDatabase, collection_name: str = 'recipes', k: int = 10):
        super().__init__()
        self._rag_db = rag_db
        self._collection_name = collection_name
        self._k = k
    
    def _get_relevant_documents(
        self, 
        query: str, 
        *, 
        run_manager: CallbackManagerForRetrieverRun
    ) -> List[Document]:
        """Retrieve relevant documents for a query"""
        try:
            # Search using RAG database
            results = self._rag_db.search_similar(
                query=query, 
                collection_name=self._collection_name, 
                n_results=self._k
            )
            
            documents = []
            
            if results['documents'][0]:
                for doc, metadata, distance in zip(
                    results['documents'][0], 
                    results['metadatas'][0], 
                    results.get('distances', [[]] * len(results['documents'][0]))[0]
                ):
                    # Add distance to metadata
                    metadata['similarity_score'] = 1 - distance if distance else 1.0
                    
                    # Create LangChain Document
                    documents.append(Document(
                        page_content=doc,
                        metadata=metadata
                    ))
            
            return documents
            
        except Exception as e:
            print(f"âŒ Error retrieving documents: {str(e)}")
            return []

class RAGChain:
    """RAG Chain combining retrieval and generation using LangChain and OpenAI"""
    
    def __init__(self, rag_db: RAGVectorDatabase, openai_api_key: str = None):
        self.rag_db = rag_db
        self.openai_api_key = openai_api_key
        
        # Initialize retrievers for different collections
        self.retrievers = {
            'recipes': ChromaDBRetriever(rag_db, 'recipes', k=5),
            'interactions': ChromaDBRetriever(rag_db, 'interactions', k=3),
            'nutrition': ChromaDBRetriever(rag_db, 'nutrition', k=4)
        }
        
        # Setup LangChain components
        self._setup_langchain()
    
    def _setup_langchain(self):
        """Setup LangChain components"""
        try:
            from langchain.chat_models import ChatOpenAI
            from langchain.prompts import ChatPromptTemplate
            from langchain.schema.runnable import RunnablePassthrough
            from langchain.schema.output_parser import StrOutputParser
            
            # Initialize OpenAI LLM
            if self.openai_api_key:
                self.llm = ChatOpenAI(
                    temperature=0.7,
                    model_name="gpt-3.5-turbo",
                    openai_api_key=self.openai_api_key
                )
            else:
                print("âš ï¸ OpenAI API key not provided, using fallback responses")
                self.llm = None
            
            # Create comprehensive prompt template
            self.prompt = ChatPromptTemplate.from_template("""
            Báº¡n lÃ  má»™t chuyÃªn gia tÆ° váº¥n áº©m thá»±c vÃ  dinh dÆ°á»¡ng chuyÃªn nghiá»‡p. Sá»­ dá»¥ng thÃ´ng tin dÆ°á»›i Ä‘Ã¢y Ä‘á»ƒ Ä‘Æ°a ra lá»i tÆ° váº¥n chi tiáº¿t vÃ  há»¯u Ã­ch.

            THÃ”NG TIN Tá»ª CÆ  Sá» Dá»® LIá»†U:
            {context}

            CÃ‚U Há»I Cá»¦A NGÆ¯á»œI DÃ™NG:
            {question}

            HÆ¯á»šNG DáºªN TRáº¢ Lá»œI:
            1. PhÃ¢n tÃ­ch nhu cáº§u cá»§a ngÆ°á»i dÃ¹ng
            2. ÄÆ°a ra gá»£i Ã½ mÃ³n Äƒn cá»¥ thá»ƒ dá»±a trÃªn thÃ´ng tin Ä‘Ã£ cÃ³
            3. Giáº£i thÃ­ch lÃ½ do táº¡i sao nhá»¯ng mÃ³n nÃ y phÃ¹ há»£p
            4. ÄÆ°a ra lá»i khuyÃªn vá» dinh dÆ°á»¡ng vÃ  cÃ¡ch cháº¿ biáº¿n
            5. Náº¿u cÃ³ thÃ´ng tin vá» tÆ°Æ¡ng tÃ¡c khÃ¡ch hÃ ng, hÃ£y tham kháº£o Ä‘á»ƒ Ä‘Æ°a ra gá»£i Ã½ phÃ¹ há»£p

            TRáº¢ Lá»œI:
            """)
            
            print("âœ… LangChain components initialized successfully")
            
        except ImportError as e:
            print(f"âš ï¸ LangChain import error: {str(e)}")
            self.llm = None
            self.prompt = None
        except Exception as e:
            print(f"âŒ Error setting up LangChain: {str(e)}")
            self.llm = None
            self.prompt = None
    
    def retrieve_context(self, query: str, include_collections: List[str] = None) -> str:
        """Retrieve relevant context from multiple collections"""
        if include_collections is None:
            include_collections = ['recipes', 'nutrition', 'interactions']
        
        all_context = []
        
        for collection_name in include_collections:
            if collection_name in self.retrievers:
                try:
                    docs = self.retrievers[collection_name]._get_relevant_documents(
                        query, run_manager=None
                    )
                    
                    if docs:
                        context_section = f"\n=== THÃ”NG TIN Tá»ª {collection_name.upper()} ===\n"
                        for i, doc in enumerate(docs[:3]):  # Limit to top 3 results per collection
                            context_section += f"{i+1}. {doc.page_content}\n"
                            if doc.metadata.get('recipe_name'):
                                context_section += f"   MÃ³n: {doc.metadata['recipe_name']}\n"
                            if doc.metadata.get('similarity_score'):
                                context_section += f"   Äá»™ tÆ°Æ¡ng Ä‘á»“ng: {doc.metadata['similarity_score']:.2f}\n"
                            context_section += "\n"
                        
                        all_context.append(context_section)
                        
                except Exception as e:
                    print(f"âš ï¸ Error retrieving from {collection_name}: {str(e)}")
        
        return "\n".join(all_context) if all_context else "KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan."
    
    def query(self, question: str, include_collections: List[str] = None) -> Dict[str, Any]:
        """Query the RAG system with a question"""
        try:
            # Retrieve relevant context
            context = self.retrieve_context(question, include_collections)
            
            if self.llm and self.prompt:
                # Use LangChain with OpenAI
                try:
                    from langchain.schema.runnable import RunnablePassthrough
                    from langchain.schema.output_parser import StrOutputParser
                    
                    # Create the chain
                    chain = (
                        {"context": lambda x: context, "question": RunnablePassthrough()}
                        | self.prompt
                        | self.llm
                        | StrOutputParser()
                    )
                    
                    # Generate response
                    response = chain.invoke(question)
                    
                    return {
                        'answer': response,
                        'context': context,
                        'source': 'langchain_openai',
                        'success': True
                    }
                    
                except Exception as e:
                    print(f"âš ï¸ LangChain error, using fallback: {str(e)}")
                    return self._generate_fallback_response(question, context)
            else:
                return self._generate_fallback_response(question, context)
                
        except Exception as e:
            print(f"âŒ Error in RAG query: {str(e)}")
            return {
                'answer': 'Xin lá»—i, Ä‘Ã£ cÃ³ lá»—i xáº£y ra khi xá»­ lÃ½ cÃ¢u há»i cá»§a báº¡n.',
                'context': '',
                'source': 'error',
                'success': False,
                'error': str(e)
            }
    
    def _generate_fallback_response(self, question: str, context: str) -> Dict[str, Any]:
        """Generate fallback response when OpenAI is not available"""
        
        # Simple keyword-based response generation
        question_lower = question.lower()
        
        fallback_response = "Dá»±a trÃªn thÃ´ng tin cÃ³ sáºµn:\n\n"
        
        if "giáº£m cÃ¢n" in question_lower:
            fallback_response += "ğŸƒâ€â™€ï¸ Äá»ƒ giáº£m cÃ¢n hiá»‡u quáº£:\n"
            fallback_response += "â€¢ Chá»n mÃ³n Äƒn Ã­t calo, nhiá»u cháº¥t xÆ¡\n"
            fallback_response += "â€¢ TÄƒng cÆ°á»ng rau xanh, protein náº¡c\n"
            fallback_response += "â€¢ Háº¡n cháº¿ Ä‘á»“ chiÃªn, ngá»t\n\n"
        
        elif "tÄƒng cÃ¢n" in question_lower:
            fallback_response += "ğŸ’ª Äá»ƒ tÄƒng cÃ¢n lÃ nh máº¡nh:\n"
            fallback_response += "â€¢ TÄƒng lÆ°á»£ng calo vá»›i thá»±c pháº©m bá»• dÆ°á»¡ng\n"
            fallback_response += "â€¢ Nhiá»u protein, carbohydrate phá»©c táº¡p\n"
            fallback_response += "â€¢ Ä‚n nhiá»u bá»¯a nhá» trong ngÃ y\n\n"
        
        elif "tiá»ƒu Ä‘Æ°á»ng" in question_lower:
            fallback_response += "ğŸ©º Cho ngÆ°á»i tiá»ƒu Ä‘Æ°á»ng:\n"
            fallback_response += "â€¢ Chá»n thá»±c pháº©m chá»‰ sá»‘ Ä‘Æ°á»ng huyáº¿t tháº¥p\n"
            fallback_response += "â€¢ Háº¡n cháº¿ Ä‘Æ°á»ng, carbohydrate Ä‘Æ¡n giáº£n\n"
            fallback_response += "â€¢ TÄƒng cháº¥t xÆ¡ tá»« rau quáº£\n\n"
        
        elif "cao huyáº¿t Ã¡p" in question_lower:
            fallback_response += "â¤ï¸ Cho ngÆ°á»i cao huyáº¿t Ã¡p:\n"
            fallback_response += "â€¢ Háº¡n cháº¿ muá»‘i vÃ  natri\n"
            fallback_response += "â€¢ TÄƒng thá»±c pháº©m giÃ u kali\n"
            fallback_response += "â€¢ Chá»n thá»±c pháº©m tÆ°Æ¡i, háº¡n cháº¿ cháº¿ biáº¿n sáºµn\n\n"
        
        elif any(word in question_lower for word in ["mang thai", "bÃ  báº§u", "thai ká»³"]):
            fallback_response += "ğŸ¤± Cho phá»¥ ná»¯ mang thai:\n"
            fallback_response += "â€¢ Bá»• sung acid folic, sáº¯t, canxi\n"
            fallback_response += "â€¢ TrÃ¡nh thá»±c pháº©m sá»‘ng, rÆ°á»£u bia\n"
            fallback_response += "â€¢ Ä‚n nhiá»u bá»¯a nhá»\n\n"
        
        elif "chay" in question_lower:
            fallback_response += "ğŸŒ± Cho ngÆ°á»i Äƒn chay:\n"
            fallback_response += "â€¢ Äa dáº¡ng nguá»“n protein thá»±c váº­t\n"
            fallback_response += "â€¢ Bá»• sung vitamin B12, sáº¯t, káº½m\n"
            fallback_response += "â€¢ Káº¿t há»£p Ä‘áº­u, háº¡t, ngÅ© cá»‘c\n\n"
        
        elif any(word in question_lower for word in ["tráº» em", "tráº» con", "bÃ©"]):
            fallback_response += "ğŸ‘¶ Cho tráº» em:\n"
            fallback_response += "â€¢ Äáº§y Ä‘á»§ protein, canxi, vitamin D\n"
            fallback_response += "â€¢ Háº¡n cháº¿ Ä‘á»“ ngá»t, Ä‘á»“ chiÃªn\n"
            fallback_response += "â€¢ Khuyáº¿n khÃ­ch Äƒn rau quáº£ Ä‘a dáº¡ng\n\n"
        
        elif any(word in question_lower for word in ["gym", "táº­p luyá»‡n", "thá»ƒ hÃ¬nh"]):
            fallback_response += "ğŸ‹ï¸â€â™‚ï¸ Cho ngÆ°á»i táº­p luyá»‡n:\n"
            fallback_response += "â€¢ TÄƒng protein: 1.6-2.2g/kg thá»ƒ trá»ng\n"
            fallback_response += "â€¢ Carbohydrate trÆ°á»›c táº­p, protein sau táº­p\n"
            fallback_response += "â€¢ Uá»‘ng Ä‘á»§ nÆ°á»›c\n\n"
        
        else:
            fallback_response += "ğŸ½ï¸ Gá»£i Ã½ chung:\n"
            fallback_response += "â€¢ Ä‚n Ä‘a dáº¡ng cÃ¡c nhÃ³m thá»±c pháº©m\n"
            fallback_response += "â€¢ CÃ¢n báº±ng protein, carbohydrate, cháº¥t bÃ©o\n"
            fallback_response += "â€¢ TÄƒng rau xanh, trÃ¡i cÃ¢y\n\n"
        
        if context and "mÃ³n" in context.lower():
            fallback_response += "ğŸ“‹ Dá»±a trÃªn dá»¯ liá»‡u cÃ³ sáºµn, há»‡ thá»‘ng tÃ¬m tháº¥y má»™t sá»‘ mÃ³n Äƒn phÃ¹ há»£p trong cÆ¡ sá»Ÿ dá»¯ liá»‡u.\n"
        
        fallback_response += "\nğŸ’¡ Äá»ƒ cÃ³ tÆ° váº¥n chi tiáº¿t hÆ¡n, vui lÃ²ng cung cáº¥p OpenAI API key."
        
        return {
            'answer': fallback_response,
            'context': context,
            'source': 'fallback',
            'success': True
        }
    
    def get_stats(self) -> Dict[str, Any]:
        """Get RAG system statistics"""
        stats = self.rag_db.get_collection_stats()
        stats['langchain_enabled'] = self.llm is not None
        stats['openai_enabled'] = self.openai_api_key is not None
        
        return stats

def main():
    """Main function to test RAG LangChain integration"""
    print("ğŸš€ Testing RAG LangChain Integration...")
    
    # Initialize RAG database
    from rag_vector_db import RAGVectorDatabase
    rag_db = RAGVectorDatabase(use_gpu=True)
    
    # Initialize RAG Chain
    rag_chain = RAGChain(rag_db)
    
    # Test queries
    test_queries = [
        "TÃ´i muá»‘n giáº£m cÃ¢n, gá»£i Ã½ mÃ³n Äƒn Ã­t calo",
        "NgÆ°á»i tiá»ƒu Ä‘Æ°á»ng nÃªn Äƒn gÃ¬?",
        "MÃ³n Äƒn tÄƒng cÃ¢n cho ngÆ°á»i gáº§y",
        "Thá»±c Ä‘Æ¡n cho bÃ  báº§u",
        "MÃ³n chay giÃ u protein"
    ]
    
    print("\nğŸ” Testing RAG queries...")
    for query in test_queries:
        print(f"\nâ“ Query: {query}")
        result = rag_chain.query(query)
        print(f"âœ… Answer: {result['answer'][:200]}...")
        print(f"ğŸ“Š Source: {result['source']}")
    
    # Display stats
    print("\nğŸ“Š RAG System Statistics:")
    stats = rag_chain.get_stats()
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    print("\nâœ… RAG LangChain integration test complete!")

if __name__ == "__main__":
    main()
