import pandas as pd
import chromadb
from chromadb.config import Settings
from typing import List, Dict, Optional
import numpy as np
from sentence_transformers import SentenceTransformer
import torch
import os
import json
from datetime import datetime

class RAGVectorDatabase:
    def __init__(self, persist_directory: str = "./chromadb_data", use_gpu: bool = True):
        """Initialize RAG Vector Database with ChromaDB and GPU acceleration"""
        self.persist_directory = persist_directory
        self.use_gpu = use_gpu and torch.cuda.is_available()
        
        # Initialize ChromaDB client
        self.client = chromadb.PersistentClient(path=persist_directory)
        
        # Initialize embedding model with GPU support
        device = 'cuda' if self.use_gpu else 'cpu'
        print(f"ğŸš€ Initializing embedding model on device: {device}")
        
        if self.use_gpu:
            print(f"ğŸ¯ GPU Info: {torch.cuda.get_device_name(0)}")
            print(f"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory // 1024**3} GB")
        
        # Use a multilingual model that works well with Vietnamese
        self.embedding_model = SentenceTransformer(
            'paraphrase-multilingual-mpnet-base-v2',
            device=device
        )
        
        # Set batch size based on device
        self.batch_size = 32 if self.use_gpu else 16
        
        # Collections for different data types
        self.collections = {}
        self._initialize_collections()
    
    def _initialize_collections(self):
        """Initialize ChromaDB collections for different data types"""
        try:
            # Food recipes collection
            self.collections['recipes'] = self.client.get_or_create_collection(
                name="food_recipes",
                metadata={"hnsw:space": "cosine"}
            )
            
            # Customer interactions collection
            self.collections['interactions'] = self.client.get_or_create_collection(
                name="customer_interactions",
                metadata={"hnsw:space": "cosine"}
            )
            
            # Nutrition knowledge collection
            self.collections['nutrition'] = self.client.get_or_create_collection(
                name="nutrition_knowledge",
                metadata={"hnsw:space": "cosine"}
            )
            
            print("âœ… ChromaDB collections initialized successfully")
            
        except Exception as e:
            print(f"âŒ Error initializing collections: {str(e)}")
            raise
    
    def chunk_text(self, text: str, chunk_size: int = 512, overlap: int = 50) -> List[str]:
        """Split text into overlapping chunks for better retrieval"""
        if not text or len(text) < chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            chunk = text[start:end]
            
            # Try to break at word boundaries
            if end < len(text):
                last_space = chunk.rfind(' ')
                if last_space > start:
                    chunk = text[start:start + last_space]
                    end = start + last_space
            
            chunks.append(chunk.strip())
            start = end - overlap
            
            if start >= len(text):
                break
        
        return chunks
    
    def add_recipes_from_csv(self, csv_file: str, batch_size: int = 100):
        """Add recipes from CSV file to ChromaDB with chunking and GPU acceleration"""
        try:
            print(f"ğŸ“ Loading recipes from {csv_file}")
            df = pd.read_csv(csv_file)
            
            print(f"ğŸ“Š Found {len(df)} recipes to process")
            
            # Process in batches for better performance
            for i in range(0, len(df), batch_size):
                batch_df = df.iloc[i:i+batch_size]
                print(f"ğŸ”„ Processing batch {i//batch_size + 1}/{(len(df)-1)//batch_size + 1}")
                
                documents = []
                metadatas = []
                ids = []
                
                for idx, row in batch_df.iterrows():
                    # Create comprehensive text for embedding
                    recipe_text = f"""
                    TÃªn mÃ³n: {row.get('recipe_name', '')}
                    Äá»™ khÃ³: {row.get('difficulty', '')}
                    Bá»¯a Äƒn: {row.get('meal_time', '')}
                    Loáº¡i dinh dÆ°á»¡ng: {row.get('nutrition_category', '')}
                    Calories Æ°á»›c tÃ­nh: {row.get('estimated_calories', '')}
                    Thá»i gian chuáº©n bá»‹: {row.get('preparation_time_minutes', '')} phÃºt
                    Sá»‘ lÆ°á»£ng nguyÃªn liá»‡u: {row.get('ingredient_count', '')}
                    GiÃ¡ Æ°á»›c tÃ­nh: {row.get('estimated_price_vnd', '')} VND
                    ÄÃ¡nh giÃ¡: {row.get('avg_rating', '')}/5
                    """
                    
                    # Chunk the text
                    chunks = self.chunk_text(recipe_text)
                    
                    for chunk_idx, chunk in enumerate(chunks):
                        documents.append(chunk)
                        metadatas.append({
                            'recipe_name': str(row.get('recipe_name', '')),
                            'recipe_url': str(row.get('recipe_url', '')),
                            'difficulty': str(row.get('difficulty', '')),
                            'meal_time': str(row.get('meal_time', '')),
                            'nutrition_category': str(row.get('nutrition_category', '')),
                            'estimated_calories': float(row.get('estimated_calories', 0)),
                            'preparation_time_minutes': float(row.get('preparation_time_minutes', 0)),
                            'ingredient_count': int(row.get('ingredient_count', 0)),
                            'estimated_price_vnd': float(row.get('estimated_price_vnd', 0)),
                            'avg_rating': float(row.get('avg_rating', 0)),
                            'chunk_index': chunk_idx,
                            'total_chunks': len(chunks),
                            'data_type': 'recipe'
                        })
                        ids.append(f"recipe_{idx}_{chunk_idx}")
                
                # Generate embeddings using GPU
                print(f"ğŸ§  Generating embeddings for {len(documents)} chunks...")
                embeddings = self.embedding_model.encode(
                    documents,
                    batch_size=32,
                    show_progress_bar=True,
                    convert_to_tensor=False
                ).tolist()
                
                # Add to ChromaDB
                self.collections['recipes'].add(
                    documents=documents,
                    metadatas=metadatas,
                    embeddings=embeddings,
                    ids=ids
                )
                
                print(f"âœ… Added batch {i//batch_size + 1} to ChromaDB")
            
            print(f"ğŸ‰ Successfully added {len(df)} recipes to ChromaDB")
            
        except Exception as e:
            print(f"âŒ Error adding recipes from CSV: {str(e)}")
            raise
    
    def add_interactions_from_csv(self, csv_file: str, batch_size: int = 100):
        """Add customer interactions from CSV to ChromaDB"""
        try:
            print(f"ğŸ“ Loading interactions from {csv_file}")
            df = pd.read_csv(csv_file)
            
            print(f"ğŸ“Š Found {len(df)} interactions to process")
            
            # Process in batches
            for i in range(0, len(df), batch_size):
                batch_df = df.iloc[i:i+batch_size]
                print(f"ğŸ”„ Processing interaction batch {i//batch_size + 1}/{(len(df)-1)//batch_size + 1}")
                
                documents = []
                metadatas = []
                ids = []
                
                for idx, row in batch_df.iterrows():
                    # Create text for interaction
                    interaction_text = f"""
                    KhÃ¡ch hÃ ng: {row.get('customer_id', '')}
                    MÃ³n Äƒn: {row.get('recipe_name', '')}
                    ÄÃ¡nh giÃ¡: {row.get('rating', '')}/5
                    Loáº¡i tÆ°Æ¡ng tÃ¡c: {row.get('interaction_type', '')}
                    NgÃ y: {row.get('interaction_date', '')}
                    BÃ¬nh luáº­n: {row.get('comment', '')}
                    Äá»™ khÃ³: {row.get('difficulty', '')}
                    Bá»¯a Äƒn: {row.get('meal_time', '')}
                    Loáº¡i dinh dÆ°á»¡ng: {row.get('nutrition_category', '')}
                    """
                    
                    documents.append(interaction_text)
                    metadatas.append({
                        'customer_id': str(row.get('customer_id', '')),
                        'recipe_name': str(row.get('recipe_name', '')),
                        'rating': float(row.get('rating', 0)),
                        'interaction_type': str(row.get('interaction_type', '')),
                        'interaction_date': str(row.get('interaction_date', '')),
                        'comment': str(row.get('comment', '')),
                        'difficulty': str(row.get('difficulty', '')),
                        'meal_time': str(row.get('meal_time', '')),
                        'nutrition_category': str(row.get('nutrition_category', '')),
                        'data_type': 'interaction'
                    })
                    ids.append(f"interaction_{idx}")
                
                # Generate embeddings
                print(f"ğŸ§  Generating embeddings for interactions...")
                embeddings = self.embedding_model.encode(
                    documents,
                    batch_size=32,
                    show_progress_bar=True,
                    convert_to_tensor=False
                ).tolist()
                
                # Add to ChromaDB
                self.collections['interactions'].add(
                    documents=documents,
                    metadatas=metadatas,
                    embeddings=embeddings,
                    ids=ids
                )
                
                print(f"âœ… Added interaction batch {i//batch_size + 1} to ChromaDB")
            
            print(f"ğŸ‰ Successfully added {len(df)} interactions to ChromaDB")
            
        except Exception as e:
            print(f"âŒ Error adding interactions from CSV: {str(e)}")
            raise
    
    def add_nutrition_knowledge(self, knowledge_data: List[Dict]):
        """Add nutrition knowledge base to ChromaDB"""
        try:
            documents = []
            metadatas = []
            ids = []
            
            for idx, knowledge in enumerate(knowledge_data):
                # Create comprehensive text
                knowledge_text = f"""
                Chá»§ Ä‘á»: {knowledge.get('topic', '')}
                Ná»™i dung: {knowledge.get('content', '')}
                Loáº¡i: {knowledge.get('category', '')}
                ThÃ´ng tin bá»• sung: {knowledge.get('additional_info', '')}
                """
                
                # Chunk the knowledge text
                chunks = self.chunk_text(knowledge_text)
                
                for chunk_idx, chunk in enumerate(chunks):
                    documents.append(chunk)
                    metadatas.append({
                        'topic': str(knowledge.get('topic', '')),
                        'category': str(knowledge.get('category', '')),
                        'content': str(knowledge.get('content', '')),
                        'additional_info': str(knowledge.get('additional_info', '')),
                        'chunk_index': chunk_idx,
                        'total_chunks': len(chunks),
                        'data_type': 'nutrition_knowledge'
                    })
                    ids.append(f"nutrition_{idx}_{chunk_idx}")
            
            # Generate embeddings
            print(f"ğŸ§  Generating embeddings for nutrition knowledge...")
            embeddings = self.embedding_model.encode(
                documents,
                batch_size=32,
                show_progress_bar=True,
                convert_to_tensor=False
            ).tolist()
            
            # Add to ChromaDB
            self.collections['nutrition'].add(
                documents=documents,
                metadatas=metadatas,
                embeddings=embeddings,
                ids=ids
            )
            
            print(f"ğŸ‰ Successfully added {len(knowledge_data)} nutrition knowledge entries")
            
        except Exception as e:
            print(f"âŒ Error adding nutrition knowledge: {str(e)}")
            raise
    
    def search_similar(self, query: str, collection_name: str = 'recipes', n_results: int = 10) -> Dict:
        """Search for similar documents using vector similarity"""
        try:
            # Generate query embedding
            query_embedding = self.embedding_model.encode([query]).tolist()[0]
            
            # Search in specified collection
            results = self.collections[collection_name].query(
                query_embeddings=[query_embedding],
                n_results=n_results,
                include=['documents', 'metadatas', 'distances']
            )
            
            return results
            
        except Exception as e:
            print(f"âŒ Error searching similar documents: {str(e)}")
            return {'documents': [[]], 'metadatas': [[]], 'distances': [[]]}
    
    def get_collection_stats(self):
        """Get statistics about the collections"""
        stats = {}
        
        try:
            for name, collection in self.collections.items():
                count = collection.count()
                stats[name] = {
                    'document_count': count,
                    'collection_name': name
                }
            
            stats['gpu_enabled'] = self.use_gpu
            stats['device'] = 'cuda' if self.use_gpu else 'cpu'
            
            return stats
            
        except Exception as e:
            print(f"âŒ Error getting collection stats: {str(e)}")
            return {}

def setup_nutrition_knowledge():
    """Setup basic nutrition knowledge base"""
    knowledge_data = [
        {
            'topic': 'Giáº£m cÃ¢n',
            'category': 'weight_management',
            'content': 'Äá»ƒ giáº£m cÃ¢n hiá»‡u quáº£, cáº§n táº¡o ra thÃ¢m há»¥t calo báº±ng cÃ¡ch Äƒn Ã­t calo hÆ¡n vÃ  tÄƒng váº­n Ä‘á»™ng. NÃªn chá»n thá»±c pháº©m Ã­t calo nhÆ°ng giÃ u cháº¥t dinh dÆ°á»¡ng nhÆ° rau xanh, trÃ¡i cÃ¢y, protein náº¡c.',
            'additional_info': 'NÃªn giáº£m 0.5-1kg/tuáº§n. TrÃ¡nh cÃ¡c cháº¿ Ä‘á»™ Äƒn quÃ¡ kháº¯t nghiá»‡t.'
        },
        {
            'topic': 'TÄƒng cÃ¢n',
            'category': 'weight_management',
            'content': 'Äá»ƒ tÄƒng cÃ¢n lÃ nh máº¡nh, cáº§n tÄƒng lÆ°á»£ng calo náº¡p vÃ o vá»›i thá»±c pháº©m giÃ u dinh dÆ°á»¡ng. Chá»n protein cháº¥t lÆ°á»£ng cao, carbohydrate phá»©c táº¡p, vÃ  cháº¥t bÃ©o lÃ nh máº¡nh.',
            'additional_info': 'NÃªn Äƒn nhiá»u bá»¯a nhá» trong ngÃ y. Káº¿t há»£p vá»›i táº­p luyá»‡n Ä‘á»ƒ tÄƒng cÆ¡ báº¯p.'
        },
        {
            'topic': 'Tiá»ƒu Ä‘Æ°á»ng',
            'category': 'medical_condition',
            'content': 'NgÆ°á»i bá»‹ tiá»ƒu Ä‘Æ°á»ng cáº§n kiá»ƒm soÃ¡t lÆ°á»£ng Ä‘Æ°á»ng vÃ  carbohydrate. NÃªn chá»n thá»±c pháº©m cÃ³ chá»‰ sá»‘ Ä‘Æ°á»ng huyáº¿t tháº¥p, nhiá»u cháº¥t xÆ¡.',
            'additional_info': 'Ä‚n Ä‘á»u Ä‘áº·n, trÃ¡nh nhá»‹n Äƒn. Theo dÃµi lÆ°á»£ng Ä‘Æ°á»ng huyáº¿t thÆ°á»ng xuyÃªn.'
        },
        {
            'topic': 'Cao huyáº¿t Ã¡p',
            'category': 'medical_condition',
            'content': 'NgÆ°á»i cao huyáº¿t Ã¡p cáº§n háº¡n cháº¿ muá»‘i vÃ  natri. TÄƒng cÆ°á»ng thá»±c pháº©m giÃ u kali nhÆ° chuá»‘i, rau xanh.',
            'additional_info': 'Háº¡n cháº¿ thá»±c pháº©m cháº¿ biáº¿n sáºµn. TÄƒng cÆ°á»ng omega-3 tá»« cÃ¡.'
        },
        {
            'topic': 'BÃ  báº§u',
            'category': 'pregnancy',
            'content': 'Phá»¥ ná»¯ mang thai cáº§n bá»• sung acid folic, sáº¯t, canxi. TrÃ¡nh rÆ°á»£u bia, cÃ  phÃª quÃ¡ nhiá»u, thá»±c pháº©m sá»‘ng.',
            'additional_info': 'Cáº§n thÃªm 300-500 calo/ngÃ y. Ä‚n nhiá»u bá»¯a nhá» Ä‘á»ƒ trÃ¡nh buá»“n nÃ´n.'
        },
        {
            'topic': 'Ä‚n chay',
            'category': 'dietary_preference',
            'content': 'NgÆ°á»i Äƒn chay cáº§n chÃº Ã½ bá»• sung protein tá»« Ä‘áº­u, háº¡t, vitamin B12, sáº¯t, káº½m.',
            'additional_info': 'Káº¿t há»£p nhiá»u loáº¡i protein thá»±c váº­t Ä‘á»ƒ cÃ³ amino acid Ä‘áº§y Ä‘á»§.'
        },
        {
            'topic': 'Tráº» em',
            'category': 'age_group',
            'content': 'Tráº» em cáº§n dinh dÆ°á»¡ng Ä‘áº§y Ä‘á»§ Ä‘á»ƒ phÃ¡t triá»ƒn. Protein, canxi, vitamin D ráº¥t quan trá»ng.',
            'additional_info': 'Háº¡n cháº¿ Ä‘á»“ ngá»t, Ä‘á»“ chiÃªn. Khuyáº¿n khÃ­ch Äƒn rau quáº£ Ä‘a dáº¡ng.'
        },
        {
            'topic': 'Táº­p gym',
            'category': 'fitness',
            'content': 'NgÆ°á»i táº­p gym cáº§n nhiá»u protein Ä‘á»ƒ xÃ¢y dá»±ng cÆ¡ báº¯p. Carbohydrate trÆ°á»›c táº­p, protein sau táº­p.',
            'additional_info': 'Nhu cáº§u protein: 1.6-2.2g/kg thá»ƒ trá»ng. Uá»‘ng Ä‘á»§ nÆ°á»›c.'
        }
    ]
    
    return knowledge_data

def main():
    """Main function to setup RAG vector database"""
    print("ğŸš€ Setting up RAG Vector Database with ChromaDB and GPU acceleration...")
    
    # Initialize RAG database
    rag_db = RAGVectorDatabase(use_gpu=True)
    
    # Add nutrition knowledge
    print("\nğŸ“š Adding nutrition knowledge base...")
    knowledge_data = setup_nutrition_knowledge()
    rag_db.add_nutrition_knowledge(knowledge_data)
    
    # Check for existing CSV files
    csv_files = [
        'customers_data.csv',
        'interactions_encoded.csv',
        'food_recipes.csv'  # You might need to create this or use existing data
    ]
    
    for csv_file in csv_files:
        if os.path.exists(csv_file):
            print(f"\nğŸ“ Processing {csv_file}...")
            if 'customer' in csv_file.lower():
                print(f"â­ï¸ Skipping customer data (will process interactions)")
            elif 'interaction' in csv_file.lower():
                rag_db.add_interactions_from_csv(csv_file)
            elif 'recipe' in csv_file.lower() or 'food' in csv_file.lower():
                rag_db.add_recipes_from_csv(csv_file)
        else:
            print(f"âš ï¸ File {csv_file} not found, skipping...")
    
    # Display statistics
    print("\nğŸ“Š Database Statistics:")
    stats = rag_db.get_collection_stats()
    for collection_name, collection_stats in stats.items():
        if isinstance(collection_stats, dict) and 'document_count' in collection_stats:
            print(f"  {collection_name}: {collection_stats['document_count']} documents")
    
    print(f"\nğŸ”¥ GPU Enabled: {stats.get('gpu_enabled', False)}")
    print(f"ğŸ–¥ï¸ Device: {stats.get('device', 'cpu')}")
    
    # Test search
    print("\nğŸ” Testing search functionality...")
    test_query = "mÃ³n Äƒn giáº£m cÃ¢n Ã­t calo"
    results = rag_db.search_similar(test_query, 'recipes', n_results=3)
    
    if results['documents'][0]:
        print(f"Search results for '{test_query}':")
        for i, (doc, meta) in enumerate(zip(results['documents'][0][:3], results['metadatas'][0][:3])):
            print(f"  {i+1}. {meta.get('recipe_name', 'Unknown')}")
    
    print("\nâœ… RAG Vector Database setup complete!")
    
    return rag_db

if __name__ == "__main__":
    main()
