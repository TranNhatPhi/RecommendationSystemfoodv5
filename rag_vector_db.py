import pandas as pd
import chromadb
from chromadb.config import Settings
from typing import List, Dict, Optional
import numpy as np
from sentence_transformers import SentenceTransformer
import torch
import os
import json
from datetime import datetime

class RAGVectorDatabase:
    def __init__(self, persist_directory: str = "./chromadb_data", use_gpu: bool = True):
        """Initialize RAG Vector Database with ChromaDB and GPU acceleration"""
        self.persist_directory = persist_directory
        self.use_gpu = use_gpu and torch.cuda.is_available()
        
        # Initialize ChromaDB client
        self.client = chromadb.PersistentClient(path=persist_directory)
        
        # Initialize embedding model with GPU support
        device = 'cuda' if self.use_gpu else 'cpu'
        print(f"üöÄ Initializing embedding model on device: {device}")
        
        if self.use_gpu:
            print(f"üéØ GPU Info: {torch.cuda.get_device_name(0)}")
            print(f"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory // 1024**3} GB")
        
        # Use a multilingual model that works well with Vietnamese
        self.embedding_model = SentenceTransformer(
            'paraphrase-multilingual-mpnet-base-v2',
            device=device
        )
        
        # Set batch size based on device
        self.batch_size = 32 if self.use_gpu else 16
        
        # Collections for different data types
        self.collections = {}
        self._initialize_collections()
    
    def _initialize_collections(self):
        """Initialize ChromaDB collections for different data types"""
        try:
            # Food recipes collection
            self.collections['recipes'] = self.client.get_or_create_collection(
                name="food_recipes",
                metadata={"hnsw:space": "cosine"}
            )
            
            # Customer interactions collection
            self.collections['interactions'] = self.client.get_or_create_collection(
                name="customer_interactions",
                metadata={"hnsw:space": "cosine"}
            )
            
            # Nutrition knowledge collection
            self.collections['nutrition'] = self.client.get_or_create_collection(
                name="nutrition_knowledge",
                metadata={"hnsw:space": "cosine"}
            )
            
            print("‚úÖ ChromaDB collections initialized successfully")
            
        except Exception as e:
            print(f"‚ùå Error initializing collections: {str(e)}")
            raise
    
    def chunk_text(self, text: str, chunk_size: int = 512, overlap: int = 50) -> List[str]:
        """Split text into overlapping chunks for better retrieval"""
        if not text or len(text) < chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            chunk = text[start:end]
            
            # Try to break at word boundaries
            if end < len(text):
                last_space = chunk.rfind(' ')
                if last_space > start:
                    chunk = text[start:start + last_space]
                    end = start + last_space
            
            chunks.append(chunk.strip())
            start = end - overlap
            
            if start >= len(text):
                break
        
        return chunks
    
    def add_recipes_from_csv(self, csv_file: str, batch_size: int = 100):
        """Add recipes from CSV file to ChromaDB with chunking and GPU acceleration"""
        try:
            print(f"üìÅ Loading recipes from {csv_file}")
            df = pd.read_csv(csv_file)
            
            print(f"üìä Found {len(df)} recipes to process")
            
            # Process in batches for better performance
            for i in range(0, len(df), batch_size):
                batch_df = df.iloc[i:i+batch_size]
                print(f"üîÑ Processing batch {i//batch_size + 1}/{(len(df)-1)//batch_size + 1}")
                
                documents = []
                metadatas = []
                ids = []
                
                for idx, row in batch_df.iterrows():
                    # Create comprehensive text for embedding
                    recipe_text = f"""
                    T√™n m√≥n: {row.get('recipe_name', '')}
                    ƒê·ªô kh√≥: {row.get('difficulty', '')}
                    B·ªØa ƒÉn: {row.get('meal_time', '')}
                    Lo·∫°i dinh d∆∞·ª°ng: {row.get('nutrition_category', '')}
                    Calories ∆∞·ªõc t√≠nh: {row.get('estimated_calories', '')}
                    Th·ªùi gian chu·∫©n b·ªã: {row.get('preparation_time_minutes', '')} ph√∫t
                    S·ªë l∆∞·ª£ng nguy√™n li·ªáu: {row.get('ingredient_count', '')}
                    Gi√° ∆∞·ªõc t√≠nh: {row.get('estimated_price_vnd', '')} VND
                    ƒê√°nh gi√°: {row.get('avg_rating', '')}/5
                    """
                    
                    # Chunk the text
                    chunks = self.chunk_text(recipe_text)
                    
                    for chunk_idx, chunk in enumerate(chunks):
                        documents.append(chunk)
                        metadatas.append({
                            'recipe_name': str(row.get('recipe_name', '')),
                            'recipe_url': str(row.get('recipe_url', '')),
                            'difficulty': str(row.get('difficulty', '')),
                            'meal_time': str(row.get('meal_time', '')),
                            'nutrition_category': str(row.get('nutrition_category', '')),
                            'estimated_calories': float(row.get('estimated_calories', 0)),
                            'preparation_time_minutes': float(row.get('preparation_time_minutes', 0)),
                            'ingredient_count': int(row.get('ingredient_count', 0)),
                            'estimated_price_vnd': float(row.get('estimated_price_vnd', 0)),
                            'avg_rating': float(row.get('avg_rating', 0)),
                            'chunk_index': chunk_idx,
                            'total_chunks': len(chunks),
                            'data_type': 'recipe'
                        })
                        ids.append(f"recipe_{idx}_{chunk_idx}")
                
                # Generate embeddings using GPU
                print(f"üß† Generating embeddings for {len(documents)} chunks...")
                embeddings = self.embedding_model.encode(
                    documents,
                    batch_size=32,
                    show_progress_bar=True,
                    convert_to_tensor=False
                ).tolist()
                
                # Add to ChromaDB
                self.collections['recipes'].add(
                    documents=documents,
                    metadatas=metadatas,
                    embeddings=embeddings,
                    ids=ids
                )
                
                print(f"‚úÖ Added batch {i//batch_size + 1} to ChromaDB")
            
            print(f"üéâ Successfully added {len(df)} recipes to ChromaDB")
            
        except Exception as e:
            print(f"‚ùå Error adding recipes from CSV: {str(e)}")
            raise
    
    def add_interactions_from_csv(self, csv_file: str, batch_size: int = 100):
        """Add customer interactions from CSV to ChromaDB"""
        try:
            print(f"üìÅ Loading interactions from {csv_file}")
            df = pd.read_csv(csv_file)
            
            print(f"üìä Found {len(df)} interactions to process")
            
            # Process in batches
            for i in range(0, len(df), batch_size):
                batch_df = df.iloc[i:i+batch_size]
                print(f"üîÑ Processing interaction batch {i//batch_size + 1}/{(len(df)-1)//batch_size + 1}")
                
                documents = []
                metadatas = []
                ids = []
                
                for idx, row in batch_df.iterrows():
                    # Create text for interaction
                    interaction_text = f"""
                    Kh√°ch h√†ng: {row.get('customer_id', '')}
                    M√≥n ƒÉn: {row.get('recipe_name', '')}
                    ƒê√°nh gi√°: {row.get('rating', '')}/5
                    Lo·∫°i t∆∞∆°ng t√°c: {row.get('interaction_type', '')}
                    Ng√†y: {row.get('interaction_date', '')}
                    B√¨nh lu·∫≠n: {row.get('comment', '')}
                    ƒê·ªô kh√≥: {row.get('difficulty', '')}
                    B·ªØa ƒÉn: {row.get('meal_time', '')}
                    Lo·∫°i dinh d∆∞·ª°ng: {row.get('nutrition_category', '')}
                    """
                    
                    documents.append(interaction_text)
                    metadatas.append({
                        'customer_id': str(row.get('customer_id', '')),
                        'recipe_name': str(row.get('recipe_name', '')),
                        'rating': float(row.get('rating', 0)),
                        'interaction_type': str(row.get('interaction_type', '')),
                        'interaction_date': str(row.get('interaction_date', '')),
                        'comment': str(row.get('comment', '')),
                        'difficulty': str(row.get('difficulty', '')),
                        'meal_time': str(row.get('meal_time', '')),
                        'nutrition_category': str(row.get('nutrition_category', '')),
                        'data_type': 'interaction'
                    })
                    ids.append(f"interaction_{idx}")
                
                # Generate embeddings
                print(f"üß† Generating embeddings for interactions...")
                embeddings = self.embedding_model.encode(
                    documents,
                    batch_size=32,
                    show_progress_bar=True,
                    convert_to_tensor=False
                ).tolist()
                
                # Add to ChromaDB
                self.collections['interactions'].add(
                    documents=documents,
                    metadatas=metadatas,
                    embeddings=embeddings,
                    ids=ids
                )
                
                print(f"‚úÖ Added interaction batch {i//batch_size + 1} to ChromaDB")
            
            print(f"üéâ Successfully added {len(df)} interactions to ChromaDB")
            
        except Exception as e:
            print(f"‚ùå Error adding interactions from CSV: {str(e)}")
            raise
    
    def add_nutrition_knowledge(self, knowledge_data: List[Dict]):
        """Add nutrition knowledge base to ChromaDB"""
        try:
            documents = []
            metadatas = []
            ids = []
            
            for idx, knowledge in enumerate(knowledge_data):
                # Create comprehensive text
                knowledge_text = f"""
                Ch·ªß ƒë·ªÅ: {knowledge.get('topic', '')}
                N·ªôi dung: {knowledge.get('content', '')}
                Lo·∫°i: {knowledge.get('category', '')}
                Th√¥ng tin b·ªï sung: {knowledge.get('additional_info', '')}
                """
                
                # Chunk the knowledge text
                chunks = self.chunk_text(knowledge_text)
                
                for chunk_idx, chunk in enumerate(chunks):
                    documents.append(chunk)
                    metadatas.append({
                        'topic': str(knowledge.get('topic', '')),
                        'category': str(knowledge.get('category', '')),
                        'content': str(knowledge.get('content', '')),
                        'additional_info': str(knowledge.get('additional_info', '')),
                        'chunk_index': chunk_idx,
                        'total_chunks': len(chunks),
                        'data_type': 'nutrition_knowledge'
                    })
                    ids.append(f"nutrition_{idx}_{chunk_idx}")
            
            # Generate embeddings
            print(f"üß† Generating embeddings for nutrition knowledge...")
            embeddings = self.embedding_model.encode(
                documents,
                batch_size=32,
                show_progress_bar=True,
                convert_to_tensor=False
            ).tolist()
            
            # Add to ChromaDB
            self.collections['nutrition'].add(
                documents=documents,
                metadatas=metadatas,
                embeddings=embeddings,
                ids=ids
            )
            
            print(f"üéâ Successfully added {len(knowledge_data)} nutrition knowledge entries")
            
        except Exception as e:
            print(f"‚ùå Error adding nutrition knowledge: {str(e)}")
            raise
    
    def search_similar(self, query: str, collection_name: str = 'recipes', n_results: int = 10) -> Dict:
        """Search for similar documents using vector similarity"""
        try:
            # Generate query embedding
            query_embedding = self.embedding_model.encode([query]).tolist()[0]
            
            # Search in specified collection
            results = self.collections[collection_name].query(
                query_embeddings=[query_embedding],
                n_results=n_results,
                include=['documents', 'metadatas', 'distances']
            )
            
            return results
            
        except Exception as e:
            print(f"‚ùå Error searching similar documents: {str(e)}")
            return {'documents': [[]], 'metadatas': [[]], 'distances': [[]]}
    
    def get_collection_stats(self):
        """Get statistics about the collections"""
        stats = {}
        
        try:
            for name, collection in self.collections.items():
                count = collection.count()
                stats[name] = {
                    'document_count': count,
                    'collection_name': name
                }
            
            stats['gpu_enabled'] = self.use_gpu
            stats['device'] = 'cuda' if self.use_gpu else 'cpu'
            
            return stats
            
        except Exception as e:
            print(f"‚ùå Error getting collection stats: {str(e)}")
            return {}

def setup_nutrition_knowledge():
    """Setup basic nutrition knowledge base"""
    knowledge_data = [
        {
            'topic': 'Gi·∫£m c√¢n',
            'category': 'weight_management',
            'content': 'ƒê·ªÉ gi·∫£m c√¢n hi·ªáu qu·∫£, c·∫ßn t·∫°o ra th√¢m h·ª•t calo b·∫±ng c√°ch ƒÉn √≠t calo h∆°n v√† tƒÉng v·∫≠n ƒë·ªông. N√™n ch·ªçn th·ª±c ph·∫©m √≠t calo nh∆∞ng gi√†u ch·∫•t dinh d∆∞·ª°ng nh∆∞ rau xanh, tr√°i c√¢y, protein n·∫°c.',
            'additional_info': 'N√™n gi·∫£m 0.5-1kg/tu·∫ßn. Tr√°nh c√°c ch·∫ø ƒë·ªô ƒÉn qu√° kh·∫Øt nghi·ªát.'
        },
        {
            'topic': 'TƒÉng c√¢n',
            'category': 'weight_management',
            'content': 'ƒê·ªÉ tƒÉng c√¢n l√†nh m·∫°nh, c·∫ßn tƒÉng l∆∞·ª£ng calo n·∫°p v√†o v·ªõi th·ª±c ph·∫©m gi√†u dinh d∆∞·ª°ng. Ch·ªçn protein ch·∫•t l∆∞·ª£ng cao, carbohydrate ph·ª©c t·∫°p, v√† ch·∫•t b√©o l√†nh m·∫°nh.',
            'additional_info': 'N√™n ƒÉn nhi·ªÅu b·ªØa nh·ªè trong ng√†y. K·∫øt h·ª£p v·ªõi t·∫≠p luy·ªán ƒë·ªÉ tƒÉng c∆° b·∫Øp.'
        },
        {
            'topic': 'Ti·ªÉu ƒë∆∞·ªùng',
            'category': 'medical_condition',
            'content': 'Ng∆∞·ªùi b·ªã ti·ªÉu ƒë∆∞·ªùng c·∫ßn ki·ªÉm so√°t l∆∞·ª£ng ƒë∆∞·ªùng v√† carbohydrate. N√™n ch·ªçn th·ª±c ph·∫©m c√≥ ch·ªâ s·ªë ƒë∆∞·ªùng huy·∫øt th·∫•p, nhi·ªÅu ch·∫•t x∆°.',
            'additional_info': 'ƒÇn ƒë·ªÅu ƒë·∫∑n, tr√°nh nh·ªãn ƒÉn. Theo d√µi l∆∞·ª£ng ƒë∆∞·ªùng huy·∫øt th∆∞·ªùng xuy√™n.'
        },
        {
            'topic': 'Cao huy·∫øt √°p',
            'category': 'medical_condition',
            'content': 'Ng∆∞·ªùi cao huy·∫øt √°p c·∫ßn h·∫°n ch·∫ø mu·ªëi v√† natri. TƒÉng c∆∞·ªùng th·ª±c ph·∫©m gi√†u kali nh∆∞ chu·ªëi, rau xanh.',
            'additional_info': 'H·∫°n ch·∫ø th·ª±c ph·∫©m ch·∫ø bi·∫øn s·∫µn. TƒÉng c∆∞·ªùng omega-3 t·ª´ c√°.'
        },
        {
            'topic': 'B√† b·∫ßu',
            'category': 'pregnancy',
            'content': 'Ph·ª• n·ªØ mang thai c·∫ßn b·ªï sung acid folic, s·∫Øt, canxi. Tr√°nh r∆∞·ª£u bia, c√† ph√™ qu√° nhi·ªÅu, th·ª±c ph·∫©m s·ªëng.',
            'additional_info': 'C·∫ßn th√™m 300-500 calo/ng√†y. ƒÇn nhi·ªÅu b·ªØa nh·ªè ƒë·ªÉ tr√°nh bu·ªìn n√¥n.'
        },
        {
            'topic': 'ƒÇn chay',
            'category': 'dietary_preference',
            'content': 'Ng∆∞·ªùi ƒÉn chay c·∫ßn ch√∫ √Ω b·ªï sung protein t·ª´ ƒë·∫≠u, h·∫°t, vitamin B12, s·∫Øt, k·∫Ωm.',
            'additional_info': 'K·∫øt h·ª£p nhi·ªÅu lo·∫°i protein th·ª±c v·∫≠t ƒë·ªÉ c√≥ amino acid ƒë·∫ßy ƒë·ªß.'
        },
        {
            'topic': 'Tr·∫ª em',
            'category': 'age_group',
            'content': 'Tr·∫ª em c·∫ßn dinh d∆∞·ª°ng ƒë·∫ßy ƒë·ªß ƒë·ªÉ ph√°t tri·ªÉn. Protein, canxi, vitamin D r·∫•t quan tr·ªçng.',
            'additional_info': 'H·∫°n ch·∫ø ƒë·ªì ng·ªçt, ƒë·ªì chi√™n. Khuy·∫øn kh√≠ch ƒÉn rau qu·∫£ ƒëa d·∫°ng.'
        },
        {
            'topic': 'T·∫≠p gym',
            'category': 'fitness',
            'content': 'Ng∆∞·ªùi t·∫≠p gym c·∫ßn nhi·ªÅu protein ƒë·ªÉ x√¢y d·ª±ng c∆° b·∫Øp. Carbohydrate tr∆∞·ªõc t·∫≠p, protein sau t·∫≠p.',
            'additional_info': 'Nhu c·∫ßu protein: 1.6-2.2g/kg th·ªÉ tr·ªçng. U·ªëng ƒë·ªß n∆∞·ªõc.'
        }
    ]
    
    return knowledge_data

def main():
    """Main function to setup RAG vector database"""
    print("üöÄ Setting up RAG Vector Database with ChromaDB and GPU acceleration...")
    
    # Initialize RAG database
    rag_db = RAGVectorDatabase(use_gpu=True)
    
    # Add nutrition knowledge
    print("\nüìö Adding nutrition knowledge base...")
    knowledge_data = setup_nutrition_knowledge()
    rag_db.add_nutrition_knowledge(knowledge_data)
    
    # Check for existing CSV files
    csv_files = [
        'customers_data.csv',
        'interactions_encoded.csv',
        'food_recipes.csv'  # You might need to create this or use existing data
    ]
    
    for csv_file in csv_files:
        if os.path.exists(csv_file):
            print(f"\nüìÅ Processing {csv_file}...")
            if 'customer' in csv_file.lower():
                print(f"‚è≠Ô∏è Skipping customer data (will process interactions)")
            elif 'interaction' in csv_file.lower():
                rag_db.add_interactions_from_csv(csv_file)
            elif 'recipe' in csv_file.lower() or 'food' in csv_file.lower():
                rag_db.add_recipes_from_csv(csv_file)
        else:
            print(f"‚ö†Ô∏è File {csv_file} not found, skipping...")
    
    # Display statistics
    print("\nüìä Database Statistics:")
    stats = rag_db.get_collection_stats()
    for collection_name, collection_stats in stats.items():
        if isinstance(collection_stats, dict) and 'document_count' in collection_stats:
            print(f"  {collection_name}: {collection_stats['document_count']} documents")
    
    print(f"\nüî• GPU Enabled: {stats.get('gpu_enabled', False)}")
    print(f"üñ•Ô∏è Device: {stats.get('device', 'cpu')}")
    
    # Test search
    print("\nüîç Testing search functionality...")
    test_query = "m√≥n ƒÉn gi·∫£m c√¢n √≠t calo"
    results = rag_db.search_similar(test_query, 'recipes', n_results=3)
    
    if results['documents'][0]:
        print(f"Search results for '{test_query}':")
        for i, (doc, meta) in enumerate(zip(results['documents'][0][:3], results['metadatas'][0][:3])):
            print(f"  {i+1}. {meta.get('recipe_name', 'Unknown')}")
    
    print("\n‚úÖ RAG Vector Database setup complete!")
    
    return rag_db

if __name__ == "__main__":
    main()
